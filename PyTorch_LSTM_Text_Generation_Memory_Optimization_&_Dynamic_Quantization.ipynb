{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rjkq52idusqZ",
        "outputId": "4d907483-368a-4870-b64c-0f0b363a54f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.2.0+cu118\n",
            "Uninstalling torch-2.2.0+cu118:\n",
            "  Successfully uninstalled torch-2.2.0+cu118\n",
            "Found existing installation: torchtext 0.17.0+cpu\n",
            "Uninstalling torchtext-0.17.0+cpu:\n",
            "  Successfully uninstalled torchtext-0.17.0+cpu\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torch-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchtext\n",
            "  Using cached https://download.pytorch.org/whl/torchtext-0.17.0%2Bcpu-cp311-cp311-linux_x86_64.whl (2.0 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.87)\n",
            "Collecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.5.86)\n",
            "Collecting nvidia-nccl-cu11==2.21.5 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.86)\n",
            "Collecting triton==3.3.0 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (75.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.32.3)\n",
            "Collecting torch\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torch-2.2.0%2Bcu118-cp311-cp311-linux_x86_64.whl (811.7 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.0.2)\n",
            "Requirement already satisfied: torchdata==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torchtext) (0.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /usr/local/lib/python3.11/dist-packages (from torch) (8.7.0.84)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.7.1->torchtext) (2.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: torch, torchtext\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.2.0+cu118 which is incompatible.\n",
            "torchtune 0.6.1 requires torchdata==0.11.0, but you have torchdata 0.7.1 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.2.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.2.0+cu118 torchtext-0.17.0+cpu\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              },
              "id": "c3b216f1bc17465ab7867fc3908893a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: portalocker>=2.0.0 in /usr/local/lib/python3.11/dist-packages (3.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall torch torchtext -y\n",
        "!pip install torch torchtext --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install 'portalocker>=2.0.0'\n",
        "!pip install 'numpy<2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0l_UBDarnXHM",
        "outputId": "1aac5c88-7b2e-4bf7-8867-0d69582f8210"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchtext.datasets import IMDB\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence # For padding\n",
        "import torch.nn.functional as F # For softmax and multinomial sampling\n",
        "import torch.quantization\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 0. Setup Global Variables and Special Tokens ---\n",
        "# Define special tokens and their indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "special_tokens = ['<unk>', '<pad>', '<bos>', '<eos>']"
      ],
      "metadata": {
        "id": "Jo-Wq6FN_6zh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "V0oUpqRcnqoB"
      },
      "outputs": [],
      "source": [
        "# --- 1. Data Loading and Preprocessing ---\n",
        "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "oD3halXCumgU"
      },
      "outputs": [],
      "source": [
        "# Build vocabulary from training data, including special tokens\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter),\n",
        "                                  min_freq=1,\n",
        "                                  specials=special_tokens,\n",
        "                                  special_first=True) # Ensure special tokens are at the beginning\n",
        "vocab.set_default_index(UNK_IDX) # Set default index for unknown tokens\n",
        "\n",
        "# Text processing pipeline: converts raw text string to a list of token IDs\n",
        "def text_pipeline(text):\n",
        "    return vocab(tokenizer(text))\n",
        "\n",
        "# Collate function for DataLoader: Pads sequences and creates input/target pairs\n",
        "# In collate_batch function, before pad_sequence\n",
        "MAX_SEQUENCE_LENGTH = 16 # Define this globally, adjust as needed\n",
        "def collate_batch(batch):\n",
        "    data = []\n",
        "    for _, text in batch:\n",
        "        token_ids = [BOS_IDX] + text_pipeline(text) + [EOS_IDX]\n",
        "        # Truncate sequences that are too long\n",
        "        if len(token_ids) > MAX_SEQUENCE_LENGTH:\n",
        "            token_ids = token_ids[:MAX_SEQUENCE_LENGTH - 1] + [EOS_IDX] # Ensure EOS is present\n",
        "        data.append(torch.tensor(token_ids, dtype=torch.long))\n",
        "    data = pad_sequence(data, batch_first=True, padding_value=PAD_IDX)\n",
        "    input_seq = data[:, :-1]\n",
        "    target_seq = data[:, 1:]\n",
        "    return input_seq, target_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "1DUKTDjHuo-t"
      },
      "outputs": [],
      "source": [
        "# --- 2. Model Definition (Text Generator) ---\n",
        "class TextGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        # Embedding layer: Converts token IDs to dense vectors\n",
        "        # `padding_idx` ensures that PAD tokens are ignored (zeroed out)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_IDX)\n",
        "        # LSTM layer: Processes sequences. `batch_first=True` matches our (batch_size, seq_len) input\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        # Linear layer: Maps LSTM output to vocabulary size (logits for next token prediction)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.init_weights()\n",
        "        self.hidden_dim = hidden_dim # Store hidden dimension for potentially initializing hidden states\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Initialize weights with a uniform distribution for better training stability\n",
        "        initrange = 0.1\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "        # LSTM weights are often initialized by PyTorch's defaults, or more sophisticated methods.\n",
        "\n",
        "    def forward(self, text, hidden=None):\n",
        "        # `text` shape: (batch_size, seq_len)\n",
        "        embedded = self.embedding(text) # Output shape: (batch_size, seq_len, embed_dim)\n",
        "        # Pass embedded sequence through LSTM.\n",
        "        # `hidden` can be passed for sequential inference (e.g., generating token by token).\n",
        "        output, hidden = self.lstm(embedded, hidden) # `output` shape: (batch_size, seq_len, hidden_dim)\n",
        "        # Apply linear layer to each time step's LSTM output\n",
        "        output = self.fc(output) # Output shape: (batch_size, seq_len, vocab_size) - logits for each token in sequence\n",
        "        return output, hidden # Return logits and the final hidden state\n",
        "\n",
        "# Model parameters\n",
        "VOCAB_SIZE = len(vocab)\n",
        "EMBED_DIM = 8\n",
        "HIDDEN_DIM = 16 # New parameter for LSTM's hidden state size\n",
        "\n",
        "model = TextGenerator(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "wlQZ-uy-u1Ui"
      },
      "outputs": [],
      "source": [
        "# --- 3. Training Setup ---\n",
        "# Create DataLoaders with the new collate_batch function\n",
        "BATCH_SIZE = 16 # Batch size for training\n",
        "train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_iter, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "# Criterion for text generation is CrossEntropyLoss, ignoring PAD tokens\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 2 # Reduced number of epochs for faster example; text generation needs more training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Training Loop ---\n",
        "scaler = GradScaler() # Initialize once"
      ],
      "metadata": {
        "id": "iskVPVb8HxTF"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "_kacn3dwu1zM"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_batches = 0 # <--- ADD THIS\n",
        "    for idx, (data, targets) in enumerate(dataloader):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            predicted_logits, _ = model(data)\n",
        "            loss = criterion(predicted_logits.view(-1, VOCAB_SIZE), targets.view(-1))\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        del data, targets, predicted_logits\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_batches += 1 # <--- ADD THIS\n",
        "\n",
        "        if idx % 100 == 0 and idx > 0:\n",
        "            print(f'Epoch {epoch}, Batch {idx}: Loss: {loss.item():.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch}: Train Loss: {total_loss/total_batches:.4f}') # <--- CHANGE THIS\n",
        "\n",
        "def evaluate(dataloader, model, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_batches = 0 # <--- ADD THIS\n",
        "    with torch.no_grad():\n",
        "        for data, targets in dataloader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "            predicted_logits, _ = model(data)\n",
        "            loss = criterion(predicted_logits.view(-1, VOCAB_SIZE), targets.view(-1))\n",
        "            total_loss += loss.item()\n",
        "            total_batches += 1 # <--- ADD THIS\n",
        "\n",
        "            del data, targets, predicted_logits\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return total_loss/total_batches # <--- CHANGE THIS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train(train_dataloader, model, criterion, optimizer, epoch)\n",
        "    val_loss = evaluate(test_dataloader, model, criterion)\n",
        "    print(f'Epoch {epoch}: Test Loss: {val_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8qORuMSAhFs",
        "outputId": "2f9a7d1b-4946-4a55-c82f-23e9645a99d0"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100: Loss: 6.6406\n",
            "Epoch 1, Batch 200: Loss: 6.5776\n",
            "Epoch 1, Batch 300: Loss: 6.4466\n",
            "Epoch 1, Batch 400: Loss: 6.8973\n",
            "Epoch 1, Batch 500: Loss: 6.5997\n",
            "Epoch 1, Batch 600: Loss: 6.1975\n",
            "Epoch 1, Batch 700: Loss: 6.5565\n",
            "Epoch 1, Batch 800: Loss: 6.2795\n",
            "Epoch 1, Batch 900: Loss: 6.4087\n",
            "Epoch 1, Batch 1000: Loss: 6.7015\n",
            "Epoch 1, Batch 1100: Loss: 6.0879\n",
            "Epoch 1, Batch 1200: Loss: 6.2047\n",
            "Epoch 1, Batch 1300: Loss: 6.3930\n",
            "Epoch 1, Batch 1400: Loss: 5.9754\n",
            "Epoch 1, Batch 1500: Loss: 5.9356\n",
            "Epoch 1: Train Loss: 6.5010\n",
            "Epoch 1: Test Loss: 6.4479\n",
            "Epoch 2, Batch 100: Loss: 6.3337\n",
            "Epoch 2, Batch 200: Loss: 6.7159\n",
            "Epoch 2, Batch 300: Loss: 6.4747\n",
            "Epoch 2, Batch 400: Loss: 6.7742\n",
            "Epoch 2, Batch 500: Loss: 6.2233\n",
            "Epoch 2, Batch 600: Loss: 6.2449\n",
            "Epoch 2, Batch 700: Loss: 6.4533\n",
            "Epoch 2, Batch 800: Loss: 6.2593\n",
            "Epoch 2, Batch 900: Loss: 6.3526\n",
            "Epoch 2, Batch 1000: Loss: 6.4142\n",
            "Epoch 2, Batch 1100: Loss: 6.3068\n",
            "Epoch 2, Batch 1200: Loss: 6.3856\n",
            "Epoch 2, Batch 1300: Loss: 6.6566\n",
            "Epoch 2, Batch 1400: Loss: 6.1955\n",
            "Epoch 2, Batch 1500: Loss: 6.2283\n",
            "Epoch 2: Train Loss: 6.3598\n",
            "Epoch 2: Test Loss: 6.3631\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "THsO-Nlxu13i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "725fdc51-9d51-4e1d-df9e-5a532d7dc455"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Applying quantization...\n",
            "Quantization complete.\n"
          ]
        }
      ],
      "source": [
        "# --- 5. Quantization (Post-Training Dynamic) ---\n",
        "print(\"\\nApplying quantization...\")\n",
        "\n",
        "# Create a copy of the model and load its trained weights for quantization\n",
        "model_to_quantize = TextGenerator(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM)\n",
        "model_to_quantize.load_state_dict(model.state_dict())\n",
        "model_to_quantize.eval() # Set to evaluation mode before quantization\n",
        "\n",
        "# Apply dynamic quantization to Linear and LSTM modules\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model_to_quantize, {nn.Linear, nn.LSTM}, dtype=torch.qint8 # Include nn.LSTM\n",
        ")\n",
        "print(\"Quantization complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "dDQyS6TZu17f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2d8b852-2d0e-487d-c5a5-a4336c2e70e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example text generation:\n"
          ]
        }
      ],
      "source": [
        "# --- 6. Text Generation Example ---\n",
        "print(\"\\nExample text generation:\")\n",
        "\n",
        "def generate_text(model, vocab, start_text, max_len=50, temperature=0.8):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    # Convert starting text to token IDs, prepending BOS\n",
        "    input_ids = [BOS_IDX] + text_pipeline(start_text)\n",
        "    generated_ids = list(input_ids)\n",
        "\n",
        "    # Initialize LSTM's hidden state (h_0, c_0) to None\n",
        "    hidden = None\n",
        "    model_device = next(model.parameters()).device\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len):\n",
        "            # For generation, feed only the *last* generated token as input\n",
        "            # This is crucial for autoregressive generation\n",
        "            current_input_tensor = torch.tensor([[generated_ids[-1]]], dtype=torch.long).to(model_device) # Shape (1, 1)\n",
        "\n",
        "            # Pass the single token and the current hidden state to the model\n",
        "            output_logits, hidden = model(current_input_tensor, hidden)\n",
        "\n",
        "            # Apply temperature to logits for creativity/randomness\n",
        "            # We care about the prediction for the single token in `current_input_tensor`\n",
        "            prediction_logits = output_logits[:, -1, :] / temperature\n",
        "            probabilities = F.softmax(prediction_logits, dim=-1) # Convert logits to probabilities\n",
        "\n",
        "            # Sample the next token from the probability distribution\n",
        "            next_token_id = torch.multinomial(probabilities, num_samples=1).item()\n",
        "\n",
        "            generated_ids.append(next_token_id) # Add the sampled token to the generated sequence\n",
        "\n",
        "            # Stop generation if EOS token is predicted\n",
        "            if next_token_id == EOS_IDX:\n",
        "                break\n",
        "\n",
        "    # Convert generated token IDs back to human-readable text\n",
        "    generated_text = ' '.join(vocab.lookup_tokens(generated_ids))\n",
        "    # Clean up special tokens for display\n",
        "    generated_text = generated_text.replace(vocab.lookup_token(BOS_IDX), '')\n",
        "    generated_text = generated_text.replace(vocab.lookup_token(EOS_IDX), '')\n",
        "    generated_text = generated_text.replace(vocab.lookup_token(PAD_IDX), '')\n",
        "    return ' '.join(generated_text.split()) # Remove any extra spaces caused by token replacement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "LzBzdmShu2B7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf412f64-825d-4768-c307-54fd2c25dcf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: 'This movie was'\n",
            "Generated (Original): 'this movie was foundationally compared one . performance t little'\n"
          ]
        }
      ],
      "source": [
        "# Test generation with the original (full precision) model\n",
        "start_phrase = \"This movie was\"\n",
        "generated_sentence = generate_text(model, vocab, start_phrase, max_len=30)\n",
        "print(f\"Prompt: '{start_phrase}'\\nGenerated (Original): '{generated_sentence}'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test generation with the quantized model\n",
        "start_phrase_quant = \"I did not like\"\n",
        "generated_sentence_quant = generate_text(quantized_model, vocab, start_phrase_quant, max_len=30)\n",
        "print(f\"Prompt: '{start_phrase_quant}'\\nGenerated (Quantized): '{generated_sentence_quant}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PN6fS439A3c6",
        "outputId": "99e80ac1-1ea2-48ff-e132-5db0d890b61d"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: 'I did not like'\n",
            "Generated (Quantized): 'i did not like air-conditioned as i is it enjoyed . this a theat four . , a stupid incognizant , , was his movie a be , ' this this was'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}